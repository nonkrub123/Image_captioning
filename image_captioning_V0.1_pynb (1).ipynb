{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKoOzzQUXTF-"
      },
      "source": [
        "#Load flickr30k_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivUexK712KFk"
      },
      "source": [
        "downgrade tensorflow to 2.10.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuRhBNOE2N1g"
      },
      "outputs": [],
      "source": [
        "!pip uninstall tensorflow -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLV7O2-Y2Q5G"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18mqUowT2pFr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bYwNNB45mKw"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input , Dense , Embedding , LSTM , Dropout , add\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from nltk.stem.snowball import stopwords\n",
        "from tensorflow.keras.models import Model\n",
        "from matplotlib import image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "#import tensorflow_text as text\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import seaborn\n",
        "import random\n",
        "import pickle\n",
        "#import spacy\n",
        "import nltk\n",
        "import gzip\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70AaKeXbCwAj"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Select the kaggle.json file you downloaded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPG0D5OrC5Af"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle #Creates a folder to store your credentials.\n",
        "!mv kaggle.json ~/.kaggle/ #Moves your Kaggle API credentials to the correct location.\n",
        "!chmod 600 ~/.kaggle/kaggle.json #Protects your credentials from being accessed by anyone else.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y-kOjblC7fT"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d adityajn105/flickr8k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFMOKjiADn6X"
      },
      "outputs": [],
      "source": [
        "!unzip /content/flickr8k.zip -d /content/flick8k_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0QhiAbVweH8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg3lFFFtJhxo"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E3vO8_7GaJ_"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/flick8k_images\n",
        "!cp -vr /content/flick8k_images /content/drive/MyDrive/flick8k_images #Clone dataset to your drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BYnlP_9XpV5"
      },
      "source": [
        "#Read file & make dictionary *function*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s4IQO1nDq-P"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc6O5GXyenKb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/ai_model_2024/image_captioning\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIAfyd1rRMyG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "import csv\n",
        "\n",
        "# Reading the file\n",
        "def read_file(path):\n",
        "    with open(path, 'r') as file:\n",
        "        return file.read().splitlines()  # `splitlines()` is a better option here\n",
        "\n",
        "test = read_file(\"/content/sample_data/test.txt\")\n",
        "print(test[:5])  # Print the first 5 lines to check\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr9dPj6iSwnN"
      },
      "outputs": [],
      "source": [
        "def get_data_dictionary(data):\n",
        "    descriptions = {}\n",
        "    for line in data:\n",
        "        try:\n",
        "            image_name, caption = line.split(',', 1)  # `split` only on the first comma\n",
        "            image_name = image_name.strip()  # Remove any extra spaces from image name\n",
        "            caption = caption.strip()  # Clean up the caption text\n",
        "\n",
        "            # Add the caption to the list of captions for the corresponding image\n",
        "            if image_name in descriptions:\n",
        "                descriptions[image_name].append(caption)\n",
        "            else:\n",
        "                descriptions[image_name] = [caption]\n",
        "        except ValueError:\n",
        "            print(f\"Skipping malformed line: {line}\")  # If there's an error in the line format\n",
        "    return descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TABAarP65q33"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Reading the file\n",
        "def read_file(path):\n",
        "    with open(path, 'r') as file:\n",
        "        return file.read().splitlines()  # `splitlines()` is a better option here\n",
        "\n",
        "# Converting image_captions data into a dict where keys = images and values = captions\n",
        "def get_data_dictionary(data):\n",
        "    descriptions = {}\n",
        "    for line in data:\n",
        "        try:\n",
        "            image_name, caption = line.split(',', 1)  # `split` only on the first comma\n",
        "            image_name = image_name.strip()  # Remove any extra spaces from image name\n",
        "            caption = caption.strip()  # Clean up the caption text\n",
        "\n",
        "            # Add the caption to the list of captions for the corresponding image\n",
        "            if image_name in descriptions:\n",
        "                descriptions[image_name].append(caption)\n",
        "            else:\n",
        "                descriptions[image_name] = [caption]\n",
        "        except ValueError:\n",
        "            print(f\"Skipping malformed line: {line}\")  # If there's an error in the line format\n",
        "    return descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHIdIhro_f88"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = read_file(\"/content/drive/MyDrive/ai_model_2024/image_captioning/captions.txt\") # caption path\n",
        "print(data[:5])  # Print the first 5 lines to check\n",
        "descriptions = get_data_dictionary(data)\n",
        "#print(list(descriptions.items())[:5])  # Print the first 5 image-caption pairs\n",
        "\n",
        "# Print a few entries to verify\n",
        "for image_name, captions in list(descriptions.items())[:5]:\n",
        "    print(f\"Image: {image_name}\")\n",
        "    for i, caption in enumerate(captions):\n",
        "        print(f\"  Caption {i+1}: {caption}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW5HzGgPEq76"
      },
      "source": [
        "#Text Preprocessing Functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiPUP3LMnVjW"
      },
      "outputs": [],
      "source": [
        "#print(remove_punc(\"KFC5$%&@(#())\"))\n",
        "#print(to_lower_case(\"KFC IS tHE BEST Fast food chain in the world\"))\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCZwzvKnTIga"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lerz57yq58QZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_punc(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def to_lower_case(text):\n",
        "    return text.lower()\n",
        "\n",
        "stopwords_list = stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "    text_words = [word for word in text.split() if (word not in stopwords_list) and (len(word) > 2)]\n",
        "    text = \" \".join(text_words)\n",
        "    return text\n",
        "\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'[0-9]', '', text)\n",
        "\n",
        "def remove_multiple_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Gathering all the text cleaning steps in one function\n",
        "def clean_text(text):\n",
        "    text = remove_punc(text)\n",
        "    text = to_lower_case(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_multiple_spaces(text)\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGrzMc6cE0Je"
      },
      "source": [
        "#Cleaning Captions(Optional Could skip if u already have clean txt fie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG_n2SugEzn-"
      },
      "outputs": [],
      "source": [
        "# Using predefined text preprocessing functions to clean the captions text\n",
        "def clean_captions(descriptions):\n",
        "    for image in descriptions.keys():\n",
        "        for index, caption in enumerate(descriptions[image]):\n",
        "            descriptions[image][index] = clean_text(caption)\n",
        "    return descriptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9cOH2qjT7nI"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "descriptions = get_data_dictionary(data)\n",
        "descriptions = clean_captions(descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHg2IjcQE6Id"
      },
      "source": [
        "#Saving the cleaned data(Optional Could skip if u already have clean txt fie)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1dwMY0p3sh-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Saving data dictionary into an external file\n",
        "def write_file(path, data):\n",
        "    lines = []\n",
        "    for image in data.keys():\n",
        "        for caption in data[image]:\n",
        "            lines.append(image + '\\t' + caption)\n",
        "    lines = '\\n'.join(lines)\n",
        "\n",
        "    with open(path, 'w') as file:\n",
        "        file.write(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogmaLe0NVrRi"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "test = \"/content/drive/MyDrive/ai_model_2024/image captioning/Dataset/flick8k_images/cleaned_data.txt\"\n",
        "clean_data = read_file(test)\n",
        "print(clean_data[:5])  # Print the first 5 lines to check\n",
        "for line in clean_data[1:21]:  # Skip header\n",
        "    print(line.split('\\t'))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN12HQXbBDFU"
      },
      "outputs": [],
      "source": [
        "path = os.path.join(\"/content/drive/MyDrive/flick8k_images\", 'cleaned_data.txt')\n",
        "write_file(path, descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cZqoTDAc9CQ"
      },
      "source": [
        "#Extract image featured using VGG16 pre-train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKMNoR0BDpqh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZjCJPyB3zlv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gc\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Importing ResNet50 model without the output layer\n",
        "features_extractor = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "features_extractor = Model(inputs=features_extractor.inputs, outputs=features_extractor.layers[-1].output)\n",
        "\n",
        "# Path to the folder containing images\n",
        "images_path = \"/content/drive/MyDrive/flick8k_images/Images\"\n",
        "images_names = sorted(os.listdir(images_path))\n",
        "\n",
        "# Directory to save the features\n",
        "features_save_dir = '/content/drive/MyDrive/flick8k_images/features'\n",
        "os.makedirs(features_save_dir, exist_ok=True)\n",
        "features_save_file = os.path.join(features_save_dir, 'features.bin')\n",
        "\n",
        "# Function to process a single image with error handling\n",
        "def process_image(img_path):\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "        img = img.resize((224, 224))  # Resize as required for ResNet50\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        img = np.array(img) / 255.0  # Normalize to [0,1] as expected by ResNet50\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract features in batches\n",
        "def preprocess_image(model, images_path, images_list, batch_size=1):\n",
        "    with open(features_save_file, 'ab') as f:  # 'ab' mode: append binary\n",
        "        total_processed = 0  # Counter for processed images\n",
        "\n",
        "        for i in range(0, len(images_list), batch_size):\n",
        "            batch_images = images_list[i:i + batch_size]\n",
        "            batch_features = {}\n",
        "\n",
        "            for img_name in batch_images:\n",
        "                img_path = os.path.join(images_path, img_name)\n",
        "                image = process_image(img_path)  # Process the image\n",
        "                if image is not None:  # Only process valid images\n",
        "                    feature = model.predict(image)  # Extract feature from model\n",
        "                    batch_features[img_name] = feature  # Store in dictionary\n",
        "                    total_processed += 1  # Increment the counter\n",
        "\n",
        "            # Save the batch of features to file\n",
        "            pickle.dump(batch_features, f)\n",
        "\n",
        "            # Print the number of processed images\n",
        "            print(f\"Processed {total_processed} images so far.\")\n",
        "\n",
        "            # Clear memory after each batch\n",
        "            del batch_images, batch_features\n",
        "            gc.collect()  # Explicitly trigger garbage collection to free memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-GTYpPLTVbR"
      },
      "outputs": [],
      "source": [
        "# Call the function\n",
        "preprocess_image(features_extractor, images_path, images_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jqPhu3Gbfrt"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Path to the features file\n",
        "features_save_file = '/content/drive/MyDrive/flick8k_images/features/features.bin'\n",
        "\n",
        "# Function to load features from a binary file\n",
        "def load_features(file_path):\n",
        "    all_features = {}\n",
        "    with open(file_path, 'rb') as f:\n",
        "        while True:\n",
        "            try:\n",
        "                batch_features = pickle.load(f)  # Load batch of features\n",
        "                all_features.update(batch_features)  # Update the dictionary with the batch\n",
        "            except EOFError:\n",
        "                break  # End of file reached\n",
        "    return all_features\n",
        "\n",
        "# Load features\n",
        "features = load_features(features_save_file)\n",
        "\n",
        "# Display keys and a sample feature vector to verify\n",
        "print(f\"Total images processed: {len(features)}\")\n",
        "for img_name, feature_vector in features.items():\n",
        "    print(f\"Image: {img_name}, Feature shape: {feature_vector.shape}\")\n",
        "    print(f\"Sample feature vector for {img_name}:\\n {feature_vector}\")\n",
        "    break  # Print for only one image as an example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTemPU_5dPzV"
      },
      "source": [
        "#Preprocessing the Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2XSW3PX6ARr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7-3KKjOR6QL"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def load_tokens(path, images):\n",
        "    tokens = defaultdict(list)  # Using defaultdict to simplify appending\n",
        "    try:\n",
        "        lines = read_file(path)\n",
        "        for line in lines:\n",
        "            img, caption = line.split('\\t')\n",
        "            if img in images:\n",
        "                tokens[img].append(\"<start> \" + caption + \" <end>\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error while loading tokens: {e}\")\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Dm3OoUJ7zx"
      },
      "outputs": [],
      "source": [
        "def read_file(path):\n",
        "    try:\n",
        "        with open(path, 'r') as file:\n",
        "            return file.read().splitlines()  # `splitlines()` is a better option here\n",
        "    except Exception as e:\n",
        "        print(f\"Error while reading file: {e}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-4MXrRyQy4R"
      },
      "source": [
        "#Splitting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2BKICiQoMpH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "# Function to read file and list all available images\n",
        "def list_images(path):\n",
        "    all_images = []\n",
        "    # Assuming `read_file` is a function that reads the file into lines\n",
        "    lines = read_file(path)\n",
        "\n",
        "    # Check if the first line is a header\n",
        "    if 'image' in lines[0].lower():\n",
        "        lines = lines[1:]  # Skip the header\n",
        "\n",
        "    for line in lines:\n",
        "        img, caption = line.split('\\t')\n",
        "        if img not in all_images:\n",
        "            all_images.append(img)\n",
        "    return all_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IscHZ_dEQl1Q"
      },
      "outputs": [],
      "source": [
        "# Path to the cleaned data\n",
        "cleaned_data_path = r\"/content/drive/MyDrive/flick8k_images/cleaned_data.txt\"\n",
        "\n",
        "# Listing all available images\n",
        "all_images_list = list_images(cleaned_data_path)\n",
        "\n",
        "# Check if we have images before proceeding\n",
        "if len(all_images_list) == 0:\n",
        "    print(\"No images found in the provided cleaned data file.\")\n",
        "else:\n",
        "    print(f\"Total images found: {len(all_images_list)}\")\n",
        "\n",
        "    # Proceed to split the dataset only if there are images\n",
        "    training_images, testing_images = train_test_split(all_images_list, test_size=0.1, shuffle=True, random_state=42)\n",
        "    cross_validation_images, testing_images = train_test_split(testing_images, test_size=0.5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Filter features to only include those that are in training_images, testing_images, and cross_validation_images\n",
        "    training_features = {img: features[img] for img in training_images if img in features}\n",
        "    testing_features = {img: features[img] for img in testing_images if img in features}\n",
        "    cross_validation_features = {img: features[img] for img in cross_validation_images if img in features}\n",
        "\n",
        "    # Loading training images captions dict\n",
        "    training_tokens = load_tokens(r\"/content/drive/MyDrive/flick8k_images/cleaned_data.txt\", training_images)\n",
        "    cross_validation_tokens = load_tokens(r\"/content/drive/MyDrive/flick8k_images/cleaned_data.txtt\", cross_validation_images)\n",
        "    testing_tokens  = load_tokens(r\"/content/drive/MyDrive/flick8k_images/cleaned_data.txt\", testing_images)\n",
        "\n",
        "    # Now you have training_tokens and features split correctly\n",
        "    print(f\"Training images: {len(training_images)}, Features: {len(training_features)}\")\n",
        "    print(f\"Testing images: {len(testing_images)}, Features: {len(testing_features)}\")\n",
        "    print(f\"Cross-validation images: {len(cross_validation_images)}, Features: {len(cross_validation_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKxa01EtQ1Kc"
      },
      "source": [
        "#Vectorizing the Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjA6wsZ3OG5a"
      },
      "outputs": [],
      "source": [
        "# extracting all captions into one list\n",
        "def fetch_captions(tokens) :\n",
        "  captions = []\n",
        "  for caps in tokens.values() :\n",
        "    [captions.append(cap) for cap in caps]\n",
        "  return captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6a8rjSWKOWU"
      },
      "outputs": [],
      "source": [
        "training_captions = fetch_captions(training_tokens)\n",
        "validation_captions = fetch_captions(cross_validation_tokens)\n",
        "testing_captions = fetch_captions(testing_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvuoJP8IR93a"
      },
      "outputs": [],
      "source": [
        "# Function to get max_length and full_text for a given token dictionary\n",
        "def analyze_captions(tokens):\n",
        "    captions = fetch_captions(tokens)  # Use your function to fetch captions from tokens\n",
        "    sentences_length = [len(caption.split()) for caption in captions]\n",
        "    max_length = max(sentences_length)  # Find the maximum caption length\n",
        "    full_text = ' '.join(captions)      # Concatenate all captions into a single string\n",
        "    return max_length, full_text\n",
        "\n",
        "# Analyze training, validation, and test captions\n",
        "train_max_length, train_full_text = analyze_captions(training_tokens)\n",
        "val_max_length, val_full_text = analyze_captions(cross_validation_tokens)\n",
        "test_max_length, test_full_text = analyze_captions(testing_tokens)\n",
        "\n",
        "# Display results for each set\n",
        "print(f\"Training set max length: {train_max_length}\")\n",
        "print(f\"Validation set max length: {val_max_length}\")\n",
        "print(f\"Test set max length: {test_max_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGDLFKEzR_5r"
      },
      "outputs": [],
      "source": [
        "# Create a TextVectorization layer and adapt it on training captions only\n",
        "text_dataset_train = tf.data.Dataset.from_tensor_slices(training_captions)\n",
        "vectorize_layer = TextVectorization(output_mode='int', output_sequence_length=train_max_length)\n",
        "vectorize_layer.adapt(text_dataset_train)\n",
        "\n",
        "# Apply vectorization to all three datasets\n",
        "training_captions_vectorized = vectorize_layer(training_captions)\n",
        "validation_captions_vectorized = vectorize_layer(validation_captions)\n",
        "testing_captions_vectorized = vectorize_layer(testing_captions)\n",
        "\n",
        "print(\"Vocabulary size:\", vectorize_layer.vocabulary_size())\n",
        "print(\"Example vectorized caption:\", training_captions_vectorized[0])\n",
        "\n",
        "# After adapting the vectorize_layer on the training data\n",
        "vocab = vectorize_layer.get_vocabulary()  # List of all vocabulary tokens\n",
        "vocab_size = len(vocab)  # Size of the vocabulary\n",
        "\n",
        "# Now you have both as variables\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Vocabulary Size:\", vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oRKWxc7SCis"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the save path in Google Drive\n",
        "save_path = '/content/drive/MyDrive/flick8k_images'\n",
        "\n",
        "#Pickle the config and weights then Save the pickle file to the save path\n",
        "pickle.dump({'config': vectorize_layer.get_config(),\n",
        "             'weights': vectorize_layer.get_weights()},\n",
        "            open(save_path, \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpogsEZhSJqu"
      },
      "outputs": [],
      "source": [
        "# tokenizing captions and saving it back to dict where keys:images and values:sequences\n",
        "training_images_sequences = {}\n",
        "i = 0\n",
        "for img , captions in training_tokens.items():\n",
        "    training_images_sequences[img] = []\n",
        "    for caption in captions :\n",
        "        sequence =  vectorize_layer(tf.constant([caption])).numpy().tolist()[0]\n",
        "        training_images_sequences[img].append(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWm7WdQpaTsC"
      },
      "outputs": [],
      "source": [
        "validate_images_sequences = {}\n",
        "i = 0\n",
        "for img , captions in cross_validation_tokens.items():\n",
        "    validate_images_sequences[img] = []\n",
        "    for caption in captions :\n",
        "        sequence =  vectorize_layer(tf.constant([caption])).numpy().tolist()[0]\n",
        "        validate_images_sequences[img].append(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HX3CLXDPwQT"
      },
      "outputs": [],
      "source": [
        "\n",
        "testing_images_sequences = {}\n",
        "i = 0\n",
        "for img , captions in testing_tokens.items():\n",
        "    testing_images_sequences[img] = []\n",
        "    for caption in captions :\n",
        "        sequence =  vectorize_layer(tf.constant([caption])).numpy().tolist()[0]\n",
        "        testing_images_sequences[img].append(sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI2r0_BNPvn7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S77487g7SQ9U"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/flick8k_images/training_images_sequenceslastest.json', 'w') as f:\n",
        "#     # Serialize the dictionary to JSON and write it to the file\n",
        "     json.dump(training_images_sequences, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU7VMYQKNBlw"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/flick8k_images/validate_images_sequenceslastest.json', 'w') as f:\n",
        "#     # Serialize the dictionary to JSON and write it to the file\n",
        "     json.dump(validate_images_sequences, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH5umD-I-3jz"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/flick8k_images/testing_images_sequenceslastest.json', 'w') as f:\n",
        "#     # Serialize the dictionary to JSON and write it to the file\n",
        "     json.dump(testing_images_sequences, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NYG-p3yERITX"
      },
      "outputs": [],
      "source": [
        "print(captions)\n",
        "print(train_max_length)\n",
        "print(vocab)\n",
        "print(training_images_sequences)\n",
        "print(validate_images_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzg4aHSNTIjx"
      },
      "source": [
        "#8-Building data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HO14bB5h-8B7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gc  # Import garbage collection\n",
        "\n",
        "# Parameters\n",
        "BATCH_SIZE = 16  # Reduced batch size to save memory\n",
        "BUFFER_SIZE = 1000\n",
        "MAX_LENGTH = train_max_length\n",
        "VOCAB_SIZE = vocab_size  # Set your actual vocab size here\n",
        "\n",
        "# Load your data\n",
        "with open('/content/drive/MyDrive/flick8k_images/training_images_sequenceslastest.json', 'r') as f:\n",
        "    training_images_sequences = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/flick8k_images/validate_images_sequenceslastest.json', 'r') as f:\n",
        "    validate_images_sequences = json.load(f)\n",
        "\n",
        "def data_generator(sequences, features, vocab_size, max_length, batch_size):  # Reduced batch size\n",
        "    while True:\n",
        "        input_1, input_2, output = [], [], []\n",
        "        for img, caps in sequences.items():\n",
        "            feature = features[img].squeeze().astype(np.float32)  # Ensure correct feature shape\n",
        "            for cap in caps:\n",
        "                caption = pad_sequences([cap], maxlen=max_length, padding='post')[0].astype(np.int32)  # Correct caption shape\n",
        "\n",
        "                # Generate targets by shifting the caption sequence\n",
        "                for i in range(1, len(caption)):\n",
        "                    input_2.append(caption[:i])\n",
        "                    output.append(caption[i])  # Append integer label instead of one-hot encoded\n",
        "                    input_1.append(feature)\n",
        "\n",
        "                # Yield batch when full\n",
        "                if len(input_1) >= batch_size:\n",
        "                    input_1_batch = np.array(input_1[:batch_size])\n",
        "                    input_2_batch = pad_sequences(input_2[:batch_size], maxlen=max_length, padding='post')\n",
        "                    output_batch = np.array(output[:batch_size])\n",
        "\n",
        "                    # Yield a tuple matching the output_signature\n",
        "                    yield ((input_1_batch, input_2_batch), output_batch)\n",
        "                    input_1, input_2, output = [], [], []  # Reset batch\n",
        "\n",
        "        # Optional: clear variables to save memory\n",
        "        del input_1, input_2, output\n",
        "        gc.collect()\n",
        "\n",
        "# Wrapping the generator in a tf.data.Dataset without batching here\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(training_images_sequences, training_features, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(None, 7, 7, 2048), dtype=tf.float32),  # Image feature shape\n",
        "         tf.TensorSpec(shape=(None, MAX_LENGTH), dtype=tf.int32)),    # Caption shape\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32)                  # Target integer label shape\n",
        "    )\n",
        ").shuffle(BUFFER_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# Create the validation dataset\n",
        "validate_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(validate_images_sequences, cross_validation_features, VOCAB_SIZE, MAX_LENGTH, BATCH_SIZE),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(None, 7, 7, 2048), dtype=tf.float32),  # Image feature shape\n",
        "         tf.TensorSpec(shape=(None, MAX_LENGTH), dtype=tf.int32)),    # Caption shape\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32)                  # Target integer label shape\n",
        "    )\n",
        ").prefetch(tf.data.AUTOTUNE)  # You can also shuffle if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xEn6s33WcJP8"
      },
      "outputs": [],
      "source": [
        "# Test the generator\n",
        "for (img_batch, cap_batch), target_batch in dataset.take(1):\n",
        "    print(\"Image batch shape:\", img_batch[0].shape)  # Should print (BATCH_SIZE, 7, 7, 2048)\n",
        "    print(\"Caption batch shape:\", cap_batch.shape)    # Should print (BATCH_SIZE, max_length)\n",
        "    print(\"Target batch shape:\", target_batch.shape)   # Should print (BATCH_SIZE, vocab_size)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fss9YUZFP2MN"
      },
      "outputs": [],
      "source": [
        "def check_keys_match(training_images_sequences, training_features):\n",
        "    # Get the keys from both dictionaries\n",
        "    token_keys = set(training_tokens.keys())\n",
        "    feature_keys = set(training_features.keys())\n",
        "\n",
        "    # Check if they match\n",
        "    if token_keys == feature_keys:\n",
        "        print(\"All keys match between training_tokens and features.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Keys do not match!\")\n",
        "\n",
        "        # Find keys that are in training_tokens but not in features\n",
        "        missing_in_features = token_keys - feature_keys\n",
        "        if missing_in_features:\n",
        "            print(f\"Missing in features: {missing_in_features}\")\n",
        "\n",
        "        # Find keys that are in features but not in training_tokens\n",
        "        missing_in_tokens = feature_keys - token_keys\n",
        "        if missing_in_tokens:\n",
        "            print(f\"Missing in training_tokens: {missing_in_tokens}\")\n",
        "\n",
        "        return False\n",
        "\n",
        "check_keys_match(training_tokens, training_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb8nw_q5J4Xb"
      },
      "source": [
        "#Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sPG3eA1J-DQ"
      },
      "source": [
        "Find max_size vocab size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLiXRxaodimz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2GED6n9IFHGD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, LSTM, Embedding, add\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IINzRnojPD5R"
      },
      "outputs": [],
      "source": [
        "\n",
        "no_of_features = 100352\n",
        "\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Add, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_transformer_model(no_of_features, max_length, vocab_size, learning_rate):\n",
        "    # Image feature input\n",
        "    image_input = Input(shape=(7, 7, 2048))\n",
        "    image_flat = Flatten()(image_input)\n",
        "    image_features = Dense(256, activation='relu')(image_flat)\n",
        "\n",
        "    # Caption input (tokenized caption sequence)\n",
        "    caption_input = Input(shape=(max_length,))\n",
        "    caption_embed = Embedding(vocab_size + 2, 300)(caption_input)\n",
        "\n",
        "    # Transformer Encoder Layer\n",
        "    transformer_output = MultiHeadAttention(num_heads=4, key_dim=256)(caption_embed, caption_embed)\n",
        "    transformer_output = LayerNormalization()(transformer_output)\n",
        "    transformer_output = Dense(256, activation='relu')(transformer_output)\n",
        "\n",
        "    # Combine image and caption features\n",
        "    combined = Add()([image_features, tf.reduce_mean(transformer_output, axis=1)])  # Using mean pooling\n",
        "    dense_1 = Dense(256, activation='relu')(combined)\n",
        "    output = Dense(vocab_size, activation='softmax')(dense_1)\n",
        "\n",
        "    # Build and compile model\n",
        "    model = Model(inputs=[image_input, caption_input], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the Transformer-based model\n",
        "captioning_model = build_transformer_model(no_of_features, train_max_length, vocab_size, 0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nriymwlk7IPF"
      },
      "outputs": [],
      "source": [
        "# prompt: plot model\n",
        "\n",
        "plot_model(captioning_model , show_shapes = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHGI7WK19_6t"
      },
      "source": [
        "#Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glPxKDteb1pH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "\n",
        "tf.debugging.enable_check_numerics()\n",
        "# Create a timestamped filename for saving the best model\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "checkpoint_filename = f'/content/drive/MyDrive/flick8k_images/best_model/{timestamp}.h5'\n",
        "\n",
        "# Set up ModelCheckpoint to save the best model\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    checkpoint_filename,\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "EPOCHS = 20\n",
        "# Fit the model\n",
        "history = captioning_model.fit(\n",
        "    dataset,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=len(training_images_sequences) // BATCH_SIZE,\n",
        "    validation_data=validate_dataset,  # Validation dataset\n",
        "    validation_steps=len(validate_images_sequences) // BATCH_SIZE,  # Validation steps\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint_callback]  # Include the checkpoint callback\n",
        ")\n",
        "\n",
        "# Save training history\n",
        "with open(f'/content/drive/MyDrive/flick8k_images/training_history_{timestamp}.json', 'w') as f:\n",
        "    json.dump(history.history, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MKxIRdruodF"
      },
      "outputs": [],
      "source": [
        "captioning_model.save('/content/drive/MyDrive/flick8k_images/best_model/next_captioning_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E673euFbE-S6"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByFfIkwOMgot"
      },
      "outputs": [],
      "source": [
        "def write_list_to_file(input_list, file_name):\n",
        "    with open(file_name, \"w\") as file:\n",
        "        for item in input_list:\n",
        "            file.write(str(item) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REkdkfqUVoS3"
      },
      "outputs": [],
      "source": [
        "print(\"Sequence:\", sequence)\n",
        "print(\"Type of elements:\", [type(s) for s in sequence])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6D5E9I9T54H"
      },
      "outputs": [],
      "source": [
        "# loading txt file into list\n",
        "def read_file_to_list(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    return [line.strip() for line in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_4yVvAVT2mQ"
      },
      "outputs": [],
      "source": [
        "model_loss = read_file_to_list(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\saved_models\\model_loss.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4UPU3hjYXLH"
      },
      "source": [
        "#Loading model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88KC370JgTM2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the ResNet50 model for feature extraction\n",
        "features_extractor = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "features_extractor = Model(inputs=features_extractor.inputs, outputs=features_extractor.layers[-1].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlETmYO0agHL"
      },
      "outputs": [],
      "source": [
        "max_length = 22\n",
        "img_path = \"/content/car.jpg\"\n",
        "\n",
        "# Function to process a single image\n",
        "def get_features_from_image(img_path,model):\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "        img = img.resize((224, 224))  # Resize as required for ResNet50\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        img = np.array(img) / 255.0  # Normalize to [0,1]\n",
        "        features = model.predict(img)\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_word(index,vocab) :\n",
        "  word = vocab[index]\n",
        "  return word\n",
        "\n",
        "def get_caption(path, features_extractor, vectorize_layer, captioning_model, max_length=22):\n",
        "    my_features = get_features_from_image(path, features_extractor)\n",
        "    caption = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequenced_caption = vectorize_layer(tf.constant([caption])).numpy().tolist()\n",
        "        padded_sequenced_caption = pad_sequences(sequenced_caption, maxlen=max_length, padding='post')[0]\n",
        "        padded_sequenced_caption = np.expand_dims(padded_sequenced_caption, axis=0)\n",
        "\n",
        "        output = captioning_model.predict([my_features, padded_sequenced_caption])\n",
        "        index = np.argmax(output[0])\n",
        "\n",
        "        # Check if predicted word is <end>\n",
        "        if index == 2:  # 2 is the index for 'end'\n",
        "            break\n",
        "        else:\n",
        "            current_word = get_word(index, vocabulary)\n",
        "            caption += ' ' + current_word\n",
        "            print(f\"Predicted word: {current_word}\")\n",
        "\n",
        "    # Remove 'start' and 'end' tokens for a clean caption\n",
        "    return caption.replace('start ', '').replace(' end', '')\n",
        "\n",
        "\n",
        "def get_caption_show_image(image_path=None) :\n",
        "    if image_path is not None :\n",
        "        image_path = image_path\n",
        "    else :\n",
        "        images_names = read_file(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\testing_images_list.txt\")\n",
        "        image_index = random.randint(0,len(images_names))\n",
        "        image_path = os.path.join(r\"E:\\Khaled\\Data\\Projects\\Image Caption Generator - GPU\\flickr30k_images\",images_names[image_index])\n",
        "\n",
        "    caption = get_caption(image_path,features_extractor,vectorize_layer,captioning_model)[8:-5]\n",
        "    print(caption)\n",
        "    image = plt.imread(image_path)\n",
        "    fig,ax = plt.subplots()\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML-zgX3fL29f"
      },
      "outputs": [],
      "source": [
        "get_caption_show_image(img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agNNRcZ2iTps"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Index of '<start>':\", vocabulary.index('<start>'))\n",
        "print(\"Index of '<end>':\", vocabulary.index('<end>'))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}